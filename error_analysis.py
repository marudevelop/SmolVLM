import json
import numpy as np 
from tqdm import tqdm
from datasets import load_dataset
from PIL import Image
import os 


try:
    from evaluate_docvqa import (
        process_answer, 
        levenshtein_distance, 
        anls_compute,
    )
except ImportError:
    print("ì˜¤ë¥˜: 'evaluate_docvqa.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ANLS ê³„ì‚°ì— í•„ìš”í•©ë‹ˆë‹¤.")
    print("ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ìˆê±°ë‚˜ PYTHONPATHì— ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    def anls_compute(gt, pred):
        print("ê²½ê³ : ì‹¤ì œ anls_compute í•¨ìˆ˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ì‹œ ê°’ 0.5ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return 0.5
    
    


RESULTS_FILE_PATH = "results/SmolVLM_DocumentVQA_validation_seed42.json"
ANALYSIS_OUTPUT_DIR = "low_score_analysis_output" # ë‚®ì€ ì ìˆ˜ ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
NUM_SAMPLES_TO_ANALYZE = 100 # ë‚®ì€ ì ìˆ˜ ìƒ˜í”Œì˜ ìˆ˜
DOCVQA_DATASET_NAME = "HuggingFaceM4/DocumentVQA"
DOCVQA_SPLIT = "validation" 

def analyze_low_score_samples():
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    IMAGE_SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_DIR, "images")
    METADATA_FILE_PATH = os.path.join(ANALYSIS_OUTPUT_DIR, "metadata.jsonl")

    # 1. JSON íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ
    try:
        with open(RESULTS_FILE_PATH, "r", encoding="utf-8") as f:
            all_results_data = json.load(f)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {RESULTS_FILE_PATH}")
        return
    except json.JSONDecodeError:
        print(f"ì˜¤ë¥˜: {RESULTS_FILE_PATH} íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return

    # 2. ê° í•­ëª©ì— ëŒ€í•´ ANLS ì ìˆ˜ ê³„ì‚°
    scored_results = []
    print("ANLS ì ìˆ˜ ê³„ì‚° ì¤‘...")
    for item in tqdm(all_results_data, desc="ANLS ì ìˆ˜ ê³„ì‚°"):
        question_id = item.get("question_id")
        predicted_raw = item.get("response")
        ground_truths_raw = item.get("answers")

        if question_id is None or predicted_raw is None or ground_truths_raw is None:
            print(f"í•­ëª© ê±´ë„ˆëœ€ (í•„ìˆ˜ í•„ë“œ ëˆ„ë½): {item.get('question_id', 'ID ì—†ìŒ')}")
            continue
        predicted_str = str(predicted_raw)
        if not ground_truths_raw:
            min_anls_distance = 1.0
        else:
            ground_truths_str_list = [str(gt) for gt in ground_truths_raw]
            anls_distances_for_gts = [anls_compute(gt_str, predicted_str) for gt_str in ground_truths_str_list]
            min_anls_distance = min(anls_distances_for_gts) if anls_distances_for_gts else 1.0
        item_score = 1.0 - min_anls_distance
        scored_results.append({
            "question_id": question_id,
            "score": item_score,
            "original_item": item
        })

    scored_results.sort(key=lambda x: x["score"])
    low_score_items_to_analyze = scored_results[:NUM_SAMPLES_TO_ANALYZE]
    
    if not low_score_items_to_analyze:
        print("ë¶„ì„í•  ë‚®ì€ ì ìˆ˜ì˜ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ ìƒìœ„ {len(low_score_items_to_analyze)}ê°œì˜ í•­ëª©ì„ ë¶„ì„í•˜ê³  ì´ë¯¸ì§€ ë° ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")

    # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    try:
        os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)
        os.makedirs(IMAGE_SAVE_DIR, exist_ok=True) # ì´ë¯¸ì§€ ì €ì¥ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
        print(f"ë¶„ì„ ê²°ê³¼ëŠ” '{ANALYSIS_OUTPUT_DIR}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
        print(f"ì´ë¯¸ì§€ëŠ” '{IMAGE_SAVE_DIR}'ì—, ë©”íƒ€ë°ì´í„°ëŠ” '{METADATA_FILE_PATH}'ì— ì €ì¥ë©ë‹ˆë‹¤.")
    except OSError as e:
        print(f"ì˜¤ë¥˜: ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ ('{ANALYSIS_OUTPUT_DIR}' ë˜ëŠ” '{IMAGE_SAVE_DIR}'): {e}")
        return

    # 5. DocumentVQA ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"{DOCVQA_DATASET_NAME} ë°ì´í„°ì…‹ ({DOCVQA_SPLIT} ìŠ¤í”Œë¦¿) ë¡œë“œ ì¤‘...")
    try:
        docvqa_dataset = load_dataset(DOCVQA_DATASET_NAME, split=DOCVQA_SPLIT, trust_remote_code=True)
    except Exception as e:
        print(f"HuggingFace ë°ì´í„°ì…‹ ({DOCVQA_DATASET_NAME}) ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ë°ì´í„°ì…‹ ì—†ì´ëŠ” ì´ë¯¸ì§€ ë° ì „ì²´ ë©”íƒ€ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
        
    print("ë°ì´í„°ì…‹ ì¸ë±ì‹± ì¤‘...")
    dataset_dict = {sample['questionId']: sample for sample in tqdm(docvqa_dataset, desc="ë°ì´í„°ì…‹ ì¸ë±ì‹±")}

    # 6 & 7. ë°ì´í„° í•„í„°ë§, ë§¤ì¹­, ì •ë³´ ì €ì¥ (ë©”íƒ€ë°ì´í„° íŒŒì¼ ë° ì´ë¯¸ì§€)
    print("\n--- ë‚®ì€ ì ìˆ˜ í•­ëª© ìƒì„¸ ì •ë³´ ì €ì¥ ì¤‘ ---")
    with open(METADATA_FILE_PATH, "w", encoding="utf-8") as metadata_f:
        for i, scored_item in enumerate(tqdm(low_score_items_to_analyze, desc="ë‚®ì€ ì ìˆ˜ í•­ëª© ì²˜ë¦¬ ë° ì €ì¥")):
            q_id = scored_item["question_id"]
            model_response = scored_item["original_item"]["response"]
            ground_truths_from_results = scored_item["original_item"]["answers"]
            original_question_text_from_json = scored_item["original_item"]["question"]
            score = scored_item["score"]

            # ê° ìƒ˜í”Œì— ëŒ€í•œ ë°ì´í„° ë ˆì½”ë“œ ì¤€ë¹„
            sample_data_record = {
                "question_id": q_id,
                "anls_score": score,
                "model_prediction": model_response,
                "model_input_question": original_question_text_from_json,
                "model_input_answers": ground_truths_from_results,
                "image_path": None, # ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ë©´ ê²½ë¡œê°€ ì±„ì›Œì§

                # ì›ë³¸ ë°ì´í„°ì…‹ì˜ í•„ë“œ (ìƒ˜í”Œ ë°œê²¬ ì‹œ ì±„ì›Œì§)
                "dataset_question": None,
                "dataset_question_types": None,
                "dataset_doc_id": None,
                "dataset_ucsf_document_id": None,
                "dataset_ucsf_document_page_no": None,
                "dataset_answers": None,
            }
            
            # í„°ë¯¸ë„ì— ê¸°ë³¸ ì •ë³´ ì¶œë ¥ (ì„ íƒ ì‚¬í•­)
            # print(f"\n{i+1}. Question ID: {q_id} (ANLS Score: {score:.4f})")
            # print(f"   ğŸ¤– ëª¨ë¸ ì‘ë‹µ: {model_response}")


            dataset_sample = dataset_dict.get(q_id)
            current_image_saved_path = None

            if dataset_sample:
                sample_data_record.update({
                    "dataset_question": dataset_sample.get("question"),
                    "dataset_question_types": dataset_sample.get("question_types"),
                    "dataset_doc_id": dataset_sample.get("docId"),
                    "dataset_ucsf_document_id": dataset_sample.get("ucsf_document_id"),
                    "dataset_ucsf_document_page_no": dataset_sample.get("ucsf_document_page_no"),
                    "dataset_answers": dataset_sample.get("answers"),
                })

                original_image = dataset_sample.get("image")
                if original_image and isinstance(original_image, Image.Image):
                    safe_q_id_str = str(q_id).replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
                    image_filename = f"image_{safe_q_id_str}.png" 
                    image_save_path = os.path.join(IMAGE_SAVE_DIR, image_filename)
                    
                    try:
                        original_image.save(image_save_path)
                        current_image_saved_path = image_save_path
                    except Exception as e:
                        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {q_id}, ê²½ë¡œ: {image_save_path}): {e}")
            else:
                print(f"   âš ï¸ ê²½ê³ : DocumentVQA ë°ì´í„°ì…‹ì—ì„œ Question ID '{q_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ IDì˜ ë°ì´í„°ì…‹ ì •ë³´ëŠ” ëˆ„ë½ë©ë‹ˆë‹¤.")
            
            sample_data_record["image_path"] = current_image_saved_path # ì´ë¯¸ì§€ ê²½ë¡œ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ None)
            

            metadata_f.write(json.dumps(sample_data_record, ensure_ascii=False) + "\n")
    print(f"\nëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” '{ANALYSIS_OUTPUT_DIR}' ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    analyze_low_score_samples()