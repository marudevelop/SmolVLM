import json
import re
import numpy as np 

# --- vqa_eval.py ë° ì œê³µëœ smp_utils.pyì˜ í—¬í¼ í•¨ìˆ˜ ë° ìƒìˆ˜ ---

MANUAL_MAP = {
    'none': '0',
    'zero': '0',
    'one': '1',
    'two': '2',
    'three': '3',
    'four': '4',
    'five': '5',
    'six': '6',
    'seven': '7',
    'eight': '8',
    'nine': '9',
    'ten': '10',
}

CONTRACTIONS = {
    'aint': "ain't", 'arent': "aren't", 'cant': "can't", 'couldve': "could've",
    'couldnt': "couldn't", "couldn'tve": "couldn't've", "couldnt've": "couldn't've",
    'didnt': "didn't", 'doesnt': "doesn't", 'dont': "don't", 'hadnt': "hadn't",
    "hadnt've": "hadn't've", "hadn'tve": "hadn't've", 'hasnt': "hasn't",
    'havent': "haven't", 'hed': "he'd", "hed've": "he'd've", "he'dve": "he'd've",
    'hes': "he's", 'howd': "how'd", 'howll': "how'll", 'hows': "how's",
    "Id've": "I'd've", "I'dve": "I'd've", 'Im': "I'm", 'Ive': "I've",
    'isnt': "isn't", 'itd': "it'd", "itd've": "it'd've", "it'dve": "it'd've",
    'itll': "it'll", "let's": "let's", 'maam': "ma'am", 'mightnt': "mightn't",
    "mightnt've": "mightn't've", "mightn'tve": "mightn't've", 'mightve': "might've",
    'mustnt': "mustn't", 'mustve': "must've", 'neednt': "needn't",
    'notve': "not've", 'oclock': "o'clock", 'oughtnt': "oughtn't",
    "ow's'at": "'ow's'at", "'ows'at": "'ow's'at", "'ow'sat": "'ow's'at",
    'shant': "shan't", "shed've": "she'd've", "she'dve": "she'd've",
    "she's": "she's", 'shouldve': "should've", 'shouldnt': "shouldn't",
    "shouldnt've": "shouldn't've", "shouldn'tve": "shouldn't've",
    "somebody'd": 'somebodyd', "somebodyd've": "somebody'd've",
    "somebody'dve": "somebody'd've", 'somebodyll': "somebody'll",
    'somebodys': "somebody's", 'someoned': "someone'd",
    "someoned've": "someone'd've", "someone'dve": "someone'd've",
    'someonell': "someone'll", 'someones': "someone's",
    'somethingd': "something'd", "somethingd've": "something'd've",
    "something'dve": "something'd've", 'somethingll': "something'll",
    'thats': "that's", 'thered': "there'd", "thered've": "there'd've",
    "there'dve": "there'd've", 'therere': "there're", 'theres': "there's",
    'theyd': "they'd", "theyd've": "they'd've", "they'dve": "they'd've",
    'theyll': "they'll", 'theyre': "they're", 'theyve': "they've",
    'twas': "'twas", 'wasnt': "wasn't", "wed've": "we'd've",
    "we'dve": "we'd've", 'weve': "we've", 'werent': "weren't",
    'whatll': "what'll", 'whatre': "what're", 'whats': "what's",
    'whatve': "what've", 'whens': "when's", 'whered': "where'd",
    'wheres': "where's", 'whereve': "where've", 'whod': "who'd",
    "whod've": "who'd've", "who'dve": "who'd've", 'wholl': "who'll",
    'whos': "who's", 'whove': "who've", 'whyll': "why'll",
    'whyre': "why're", 'whys': "why's", 'wont': "won't",
    'wouldve': "would've", 'wouldnt': "wouldn't",
    "wouldnt've": "wouldn't've", "wouldn'tve": "wouldn't've",
    'yall': "y'all", "yall'll": "y'all'll", "y'allll": "y'all'll",
    "yall'd've": "y'all'd've", "y'alld've": "y'all'd've",
    "y'all'dve": "y'all'd've", 'youd': "you'd", "youd've": "you'd've",
    "you'dve": "you'd've", 'youll': "you'll", 'youre': "you're",
    'youve': "you've"
}

def _process_digit_article(in_text: str) -> str:
    """ìˆ«ìì™€ ê´€ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë‚´ë¶€ì ìœ¼ë¡œ ì†Œë¬¸ì ë³€í™˜ í¬í•¨)"""
    out_text = []
    temp_text = in_text.lower().split()
    articles = ['a', 'an', 'the']
    for word in temp_text:
        word = MANUAL_MAP.setdefault(word, word)
        if word not in articles:
            out_text.append(word)
    for word_id, word in enumerate(out_text):
        if word in CONTRACTIONS:
            out_text[word_id] = CONTRACTIONS[word]
    return ' '.join(out_text)

def process_punctuation(inText: str) -> str:
    """ì œê³µëœ smp_utils.pyì˜ êµ¬ë‘ì  ì²˜ë¦¬ ë¡œì§."""
    outText = inText
    punct = [
        ';', r'/', '[', ']', '"', '{', '}', '(', ')', '=', '+', '\\', '_', '-',
        '>', '<', '@', '`', ',', '?', '!'
    ]
    commaStrip  = re.compile(r'(\d)(,)(\d)')
    periodStrip = re.compile(r'(?<!\d)\.(?!\d)')
    for p in punct:
        if (p + ' ' in outText or ' ' + p in outText) or \
           (re.search(commaStrip, outText) is not None):
            outText = outText.replace(p, '')
        else:
            outText = outText.replace(p, ' ')
    outText = periodStrip.sub('', outText, re.UNICODE)
    return outText

def process_answer_for_anls(answer: str) -> str:
    """ANLS ê³„ì‚°ì„ ìœ„í•´ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    if answer is None:
        return ""
    answer = str(answer)
    answer = answer.replace('\n', ' ').replace('\t', ' ')
    answer = answer.strip()
    answer = process_punctuation(answer)
    answer = _process_digit_article(answer)
    return answer

def levenshtein_distance(s1: str, s2: str) -> int:
    """ë‘ ë¬¸ìì—´ ê°„ì˜ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if len(s1) > len(s2):
        s1, s2 = s2, s1
    distances = range(len(s1) + 1)
    for i2, c2 in enumerate(s2):
        distances_ = [i2 + 1]
        for i1, c1 in enumerate(s1):
            if c1 == c2:
                distances_.append(distances[i1])
            else:
                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))
        distances = distances_
    return distances[-1]

def anls_compute(groundtruth: str, prediction: str) -> float:
    """ì •ê·œí™”ëœ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬(ANLSì˜ ê¸°ë°˜)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    gt_answer = ' '.join(groundtruth.strip().lower().split())
    det_answer = ' '.join(prediction.strip().lower().split())
    dist = levenshtein_distance(gt_answer, det_answer)
    length = max(len(gt_answer), len(det_answer))
    if length == 0:
        return 0.0 if dist == 0 else 1.0
    return float(dist) / float(length)

def evaluate_docvqa(results_file: str, model_name: str = "SmolVLM", similarity_threshold: float = 0.5):
    """
    DocVQA ê²°ê³¼ë¥¼ VLMEvalKitì˜ ANLS ì ìˆ˜ ê³„ì‚° ë°©ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤.
    ì „ì²´ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    similarity_threshold: 1 - ANLS_distance ì— ëŒ€í•œ ì„ê³„ê°’ 
    """
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ê²°ê³¼ íŒŒì¼ '{results_file}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except json.JSONDecodeError:
        print(f"ì˜¤ë¥˜: '{results_file}'ì—ì„œ JSONì„ ë””ì½”ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    total_items = 0
    all_item_scores = []
    
    print(f"=== DocVQA ANLS í‰ê°€ ê²°ê³¼ (ì „ì²´ ì‘ë‹µ ê¸°ì¤€, ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}) ===\n")
    
    for item in results:
        if model_name not in item.get("model_response", {}):
            continue
            
        total_items += 1
        
        question_id = item.get("question_id", "N/A")
        question = item.get("question", "N/A")
        ground_truths = item.get("answers", [])
        model_response_data = item.get("model_response", {}).get(model_name, {})
        model_response_full = model_response_data.get("response", "")

        # ì „ì²´ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì •ê·œì‹ìœ¼ë¡œ ìë¥´ì§€ ì•ŠìŒ)
        predicted_raw = model_response_full if model_response_full else ""
        
        processed_predicted = process_answer_for_anls(predicted_raw)
        
        min_anls_dist_for_item = 1.0  
        if ground_truths:
            current_item_anls_distances = []
            for gt_raw in ground_truths:
                processed_gt = process_answer_for_anls(str(gt_raw))
                if not processed_gt and not processed_predicted:
                    anls_dist = 0.0
                elif (not processed_gt and processed_predicted) or \
                     (processed_gt and not processed_predicted):
                    anls_dist = 1.0
                else:
                    anls_dist = anls_compute(processed_gt, processed_predicted)
                current_item_anls_distances.append(anls_dist)
            
            if current_item_anls_distances: # ANLS ê±°ë¦¬ê°€ í•˜ë‚˜ë¼ë„ ê³„ì‚°ëœ ê²½ìš°
                min_anls_dist_for_item = min(current_item_anls_distances)
                
        
        similarity = 1.0 - min_anls_dist_for_item
        item_score = 0.0 if similarity < similarity_threshold else similarity
        all_item_scores.append(item_score)

        # if item_score < similarity_threshold:
        #     print(f"âš ï¸ ID: {question_id} (ë‚®ì€ ì ìˆ˜)")
        #     print(f"   ì§ˆë¬¸: {question}")
        #     print(f"   ì •ë‹µ(Ground Truths): {ground_truths}")
        #     print(f"   ì˜ˆì¸¡(ì „ì²´ ì‘ë‹µ): '{predicted_raw[:200]}{'...' if len(predicted_raw) > 200 else ''}'")
        #     print(f"   ìµœì†Œ ANLS ê±°ë¦¬: {min_anls_dist_for_item:.4f}")
        #     print(f"   í•­ëª© ì ìˆ˜: {item_score:.4f} (ìœ ì‚¬ë„: {similarity:.4f})")
        #     print()
            
    overall_score = 0.0
    if total_items > 0:
        overall_score = np.mean(all_item_scores) * 100 
        
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   ì´ ì²˜ë¦¬ëœ ì§ˆë¬¸ ìˆ˜: {total_items}")
    print(f"   ì „ì²´ ì ìˆ˜ (VLMEvalKit DocVQA ë°©ì‹, ì „ì²´ ì‘ë‹µ ê¸°ì¤€): {overall_score:.2f}") 
    
    return {
        "total_questions": total_items,
        "overall_score": overall_score,
        "similarity_threshold": similarity_threshold 
    }

if __name__ == "__main__":
    results_file = "results/docvqa_results_exact2paper.json"  
    model_name_to_eval = "SmolVLM"                

    similarity_eval_threshold = 0.5               
    
    print(f"ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_name_to_eval}, ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_eval_threshold}")
    evaluation_metrics = evaluate_docvqa(results_file, model_name_to_eval, similarity_eval_threshold)
    
    if evaluation_metrics:
        print("\ní‰ê°€ ì™„ë£Œ.")
        print(f"ì „ì²´ ì ìˆ˜ (ìœ ì‚¬ë„ ì„ê³„ê°’ {evaluation_metrics['similarity_threshold']}): {evaluation_metrics['overall_score']:.2f}")